{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 6: CI/CD e Automação de Pipelines\n",
    "\n",
    "## Objetivos de Aprendizagem\n",
    "- Criar pipelines de ML automatizados\n",
    "- Implementar versionamento de dados e código\n",
    "- Automatizar testes de modelos\n",
    "- Integrar MLFlow com CI/CD\n",
    "- Implementar validação automatizada\n",
    "\n",
    "## Exercício Prático\n",
    "Construir pipeline completo de ML com automação e testes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuração do Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pipeline de Dados\n",
    "\n",
    "### Tarefa 1: Crie pipeline de ingestão e validação de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline:\n",
    "    \"\"\"Pipeline de processamento de dados\"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42):\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.data_hash = None\n",
    "        self.validation_results = {}\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Carregar dados\"\"\"\n",
    "        print(\"[1/5] Carregando dados...\")\n",
    "        data = load_diabetes()\n",
    "        X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "        y = data.target\n",
    "        return X, y\n",
    "    \n",
    "    def validate_data(self, X, y):\n",
    "        \"\"\"Validar qualidade dos dados\"\"\"\n",
    "        print(\"[2/5] Validando dados...\")\n",
    "        validations = {}\n",
    "        \n",
    "        # Verificar valores nulos\n",
    "        null_count = X.isnull().sum().sum()\n",
    "        validations['no_nulls'] = null_count == 0\n",
    "        \n",
    "        # Verificar dimensões\n",
    "        validations['correct_shape'] = len(X) == len(y)\n",
    "        validations['min_samples'] = len(X) >= 100\n",
    "        \n",
    "        # Verificar tipos de dados\n",
    "        validations['numeric_features'] = all(X.dtypes.apply(lambda x: np.issubdtype(x, np.number)))\n",
    "        \n",
    "        # Verificar outliers extremos\n",
    "        z_scores = np.abs((X - X.mean()) / X.std())\n",
    "        extreme_outliers = (z_scores > 5).sum().sum()\n",
    "        validations['no_extreme_outliers'] = extreme_outliers == 0\n",
    "        \n",
    "        self.validation_results = validations\n",
    "        all_passed = all(validations.values())\n",
    "        \n",
    "        print(f\"  Validações: {sum(validations.values())}/{len(validations)} passed\")\n",
    "        return all_passed, validations\n",
    "    \n",
    "    def compute_data_hash(self, X, y):\n",
    "        \"\"\"Computar hash dos dados para versionamento\"\"\"\n",
    "        print(\"[3/5] Computando hash dos dados...\")\n",
    "        data_str = str(X.values.tobytes()) + str(y.tobytes())\n",
    "        self.data_hash = hashlib.md5(data_str.encode()).hexdigest()[:8]\n",
    "        print(f\"  Data hash: {self.data_hash}\")\n",
    "        return self.data_hash\n",
    "    \n",
    "    def split_data(self, X, y):\n",
    "        \"\"\"Dividir dados em treino e teste\"\"\"\n",
    "        print(\"[4/5] Dividindo dados...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state\n",
    "        )\n",
    "        print(f\"  Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def execute(self):\n",
    "        \"\"\"Executar pipeline completo\"\"\"\n",
    "        print(\"\\n=== EXECUTANDO DATA PIPELINE ===\")\n",
    "        \n",
    "        # Carregar\n",
    "        X, y = self.load_data()\n",
    "        \n",
    "        # Validar\n",
    "        valid, validations = self.validate_data(X, y)\n",
    "        if not valid:\n",
    "            raise ValueError(f\"Validação de dados falhou: {validations}\")\n",
    "        \n",
    "        # Hash\n",
    "        data_hash = self.compute_data_hash(X, y)\n",
    "        \n",
    "        # Split\n",
    "        X_train, X_test, y_train, y_test = self.split_data(X, y)\n",
    "        \n",
    "        print(\"[5/5] Pipeline de dados concluído!\\n\")\n",
    "        \n",
    "        return {\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test,\n",
    "            'data_hash': data_hash,\n",
    "            'validations': validations\n",
    "        }\n",
    "\n",
    "# Executar pipeline\n",
    "data_pipeline = DataPipeline()\n",
    "data = data_pipeline.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline de Treinamento\n",
    "\n",
    "### Tarefa 2: Crie pipeline de treinamento automatizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPipeline:\n",
    "    \"\"\"Pipeline de treinamento de modelo\"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_name=\"cicd_pipeline\"):\n",
    "        self.experiment_name = experiment_name\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    def create_model(self, params):\n",
    "        \"\"\"Criar modelo com pipeline\"\"\"\n",
    "        print(\"[1/4] Criando modelo...\")\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('regressor', RandomForestRegressor(**params))\n",
    "        ])\n",
    "        return pipeline\n",
    "    \n",
    "    def train_model(self, pipeline, X_train, y_train):\n",
    "        \"\"\"Treinar modelo\"\"\"\n",
    "        print(\"[2/4] Treinando modelo...\")\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        print(\"  Treinamento concluído\")\n",
    "        return pipeline\n",
    "    \n",
    "    def evaluate_model(self, pipeline, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Avaliar modelo\"\"\"\n",
    "        print(\"[3/4] Avaliando modelo...\")\n",
    "        \n",
    "        # Previsões\n",
    "        y_train_pred = pipeline.predict(X_train)\n",
    "        y_test_pred = pipeline.predict(X_test)\n",
    "        \n",
    "        # Métricas\n",
    "        metrics = {\n",
    "            'train_rmse': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "            'test_rmse': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "            'train_r2': r2_score(y_train, y_train_pred),\n",
    "            'test_r2': r2_score(y_test, y_test_pred),\n",
    "            'train_mae': mean_absolute_error(y_train, y_train_pred),\n",
    "            'test_mae': mean_absolute_error(y_test, y_test_pred)\n",
    "        }\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(\n",
    "            pipeline, X_train, y_train, \n",
    "            cv=5, scoring='neg_mean_squared_error'\n",
    "        )\n",
    "        metrics['cv_rmse_mean'] = np.sqrt(-cv_scores.mean())\n",
    "        metrics['cv_rmse_std'] = np.sqrt(cv_scores.std())\n",
    "        \n",
    "        print(f\"  Test RMSE: {metrics['test_rmse']:.2f}\")\n",
    "        print(f\"  Test R²: {metrics['test_r2']:.4f}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def execute(self, X_train, X_test, y_train, y_test, params, data_hash):\n",
    "        \"\"\"Executar pipeline de treinamento\"\"\"\n",
    "        print(\"\\n=== EXECUTANDO TRAINING PIPELINE ===\")\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"automated_training_{data_hash}\") as run:\n",
    "            # Criar e treinar\n",
    "            pipeline = self.create_model(params)\n",
    "            pipeline = self.train_model(pipeline, X_train, y_train)\n",
    "            \n",
    "            # Avaliar\n",
    "            metrics = self.evaluate_model(pipeline, X_train, y_train, X_test, y_test)\n",
    "            \n",
    "            # Registrar no MLFlow\n",
    "            print(\"[4/4] Registrando no MLFlow...\")\n",
    "            mlflow.log_params(params)\n",
    "            mlflow.log_params({'data_hash': data_hash})\n",
    "            mlflow.log_metrics(metrics)\n",
    "            mlflow.sklearn.log_model(pipeline, \"model\")\n",
    "            \n",
    "            print(\"Pipeline de treinamento concluído!\\n\")\n",
    "            \n",
    "            return {\n",
    "                'pipeline': pipeline,\n",
    "                'metrics': metrics,\n",
    "                'run_id': run.info.run_id\n",
    "            }\n",
    "\n",
    "# Executar pipeline de treinamento\n",
    "training_pipeline = TrainingPipeline()\n",
    "params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 10,\n",
    "    'random_state': 42\n",
    "}\n",
    "training_result = training_pipeline.execute(\n",
    "    data['X_train'], data['X_test'], \n",
    "    data['y_train'], data['y_test'],\n",
    "    params, data['data_hash']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline de Testes\n",
    "\n",
    "### Tarefa 3: Implemente testes automatizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTestPipeline:\n",
    "    \"\"\"Pipeline de testes de modelo\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.test_results = {}\n",
    "    \n",
    "    def test_model_performance(self, metrics, thresholds):\n",
    "        \"\"\"Testar performance do modelo\"\"\"\n",
    "        tests = {}\n",
    "        tests['test_r2_above_threshold'] = metrics['test_r2'] >= thresholds['min_r2']\n",
    "        tests['test_rmse_below_threshold'] = metrics['test_rmse'] <= thresholds['max_rmse']\n",
    "        tests['no_overfitting'] = abs(metrics['train_r2'] - metrics['test_r2']) <= thresholds['max_overfit_gap']\n",
    "        return tests\n",
    "    \n",
    "    def test_model_stability(self, metrics):\n",
    "        \"\"\"Testar estabilidade do modelo\"\"\"\n",
    "        tests = {}\n",
    "        tests['cv_low_variance'] = metrics['cv_rmse_std'] < metrics['cv_rmse_mean'] * 0.2\n",
    "        return tests\n",
    "    \n",
    "    def test_model_predictions(self, model, X_test, y_test):\n",
    "        \"\"\"Testar previsões do modelo\"\"\"\n",
    "        tests = {}\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Verificar se previsões são válidas\n",
    "        tests['predictions_not_null'] = not np.any(np.isnan(predictions))\n",
    "        tests['predictions_not_inf'] = not np.any(np.isinf(predictions))\n",
    "        \n",
    "        # Verificar range das previsões\n",
    "        y_range = y_test.max() - y_test.min()\n",
    "        pred_range = predictions.max() - predictions.min()\n",
    "        tests['predictions_reasonable_range'] = abs(pred_range - y_range) / y_range < 0.5\n",
    "        \n",
    "        return tests\n",
    "    \n",
    "    def test_model_interface(self, model, X_test):\n",
    "        \"\"\"Testar interface do modelo\"\"\"\n",
    "        tests = {}\n",
    "        \n",
    "        # Testar predict\n",
    "        try:\n",
    "            _ = model.predict(X_test.iloc[:1])\n",
    "            tests['predict_single_sample'] = True\n",
    "        except:\n",
    "            tests['predict_single_sample'] = False\n",
    "        \n",
    "        # Testar batch predict\n",
    "        try:\n",
    "            _ = model.predict(X_test)\n",
    "            tests['predict_batch'] = True\n",
    "        except:\n",
    "            tests['predict_batch'] = False\n",
    "        \n",
    "        return tests\n",
    "    \n",
    "    def execute(self, model, metrics, X_test, y_test):\n",
    "        \"\"\"Executar todos os testes\"\"\"\n",
    "        print(\"\\n=== EXECUTANDO MODEL TEST PIPELINE ===\")\n",
    "        \n",
    "        thresholds = {\n",
    "            'min_r2': 0.3,\n",
    "            'max_rmse': 100,\n",
    "            'max_overfit_gap': 0.15\n",
    "        }\n",
    "        \n",
    "        all_tests = {}\n",
    "        \n",
    "        # Performance tests\n",
    "        print(\"[1/4] Testando performance...\")\n",
    "        perf_tests = self.test_model_performance(metrics, thresholds)\n",
    "        all_tests.update({f'perf_{k}': v for k, v in perf_tests.items()})\n",
    "        \n",
    "        # Stability tests\n",
    "        print(\"[2/4] Testando estabilidade...\")\n",
    "        stab_tests = self.test_model_stability(metrics)\n",
    "        all_tests.update({f'stab_{k}': v for k, v in stab_tests.items()})\n",
    "        \n",
    "        # Prediction tests\n",
    "        print(\"[3/4] Testando previsões...\")\n",
    "        pred_tests = self.test_model_predictions(model, X_test, y_test)\n",
    "        all_tests.update({f'pred_{k}': v for k, v in pred_tests.items()})\n",
    "        \n",
    "        # Interface tests\n",
    "        print(\"[4/4] Testando interface...\")\n",
    "        intf_tests = self.test_model_interface(model, X_test)\n",
    "        all_tests.update({f'intf_{k}': v for k, v in intf_tests.items()})\n",
    "        \n",
    "        self.test_results = all_tests\n",
    "        \n",
    "        # Resumo\n",
    "        total_tests = len(all_tests)\n",
    "        passed_tests = sum(all_tests.values())\n",
    "        \n",
    "        print(f\"\\nResultado: {passed_tests}/{total_tests} testes passaram\")\n",
    "        \n",
    "        # Mostrar testes que falharam\n",
    "        failed = {k: v for k, v in all_tests.items() if not v}\n",
    "        if failed:\n",
    "            print(\"\\n⚠️  Testes que falharam:\")\n",
    "            for test_name in failed.keys():\n",
    "                print(f\"  - {test_name}\")\n",
    "        else:\n",
    "            print(\"\\n✅ Todos os testes passaram!\")\n",
    "        \n",
    "        return {\n",
    "            'all_passed': all(all_tests.values()),\n",
    "            'passed_count': passed_tests,\n",
    "            'total_count': total_tests,\n",
    "            'results': all_tests\n",
    "        }\n",
    "\n",
    "# Executar testes\n",
    "test_pipeline = ModelTestPipeline()\n",
    "test_results = test_pipeline.execute(\n",
    "    training_result['pipeline'],\n",
    "    training_result['metrics'],\n",
    "    data['X_test'],\n",
    "    data['y_test']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pipeline de Deployment\n",
    "\n",
    "### Tarefa 4: Automatize o deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeploymentPipeline:\n",
    "    \"\"\"Pipeline de deployment automatizado\"\"\"\n",
    "    \n",
    "    def __init__(self, model_dir=\"/tmp/models\"):\n",
    "        self.model_dir = model_dir\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    def validate_for_deployment(self, test_results):\n",
    "        \"\"\"Validar se modelo está pronto para deployment\"\"\"\n",
    "        print(\"[1/4] Validando para deployment...\")\n",
    "        \n",
    "        if not test_results['all_passed']:\n",
    "            print(\"  ❌ Modelo reprovou em testes\")\n",
    "            return False\n",
    "        \n",
    "        print(\"  ✅ Modelo aprovado em todos os testes\")\n",
    "        return True\n",
    "    \n",
    "    def package_model(self, model, metadata):\n",
    "        \"\"\"Empacotar modelo para deployment\"\"\"\n",
    "        print(\"[2/4] Empacotando modelo...\")\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_name = f\"model_{metadata['data_hash']}_{timestamp}\"\n",
    "        \n",
    "        # Salvar modelo\n",
    "        model_path = os.path.join(self.model_dir, f\"{model_name}.pkl\")\n",
    "        joblib.dump(model, model_path)\n",
    "        \n",
    "        # Salvar metadata\n",
    "        metadata_path = os.path.join(self.model_dir, f\"{model_name}_metadata.json\")\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"  Modelo salvo: {model_name}\")\n",
    "        \n",
    "        return {\n",
    "            'model_path': model_path,\n",
    "            'metadata_path': metadata_path,\n",
    "            'model_name': model_name\n",
    "        }\n",
    "    \n",
    "    def create_deployment_manifest(self, package_info, test_results, metrics):\n",
    "        \"\"\"Criar manifesto de deployment\"\"\"\n",
    "        print(\"[3/4] Criando manifesto de deployment...\")\n",
    "        \n",
    "        manifest = {\n",
    "            'deployment_timestamp': datetime.now().isoformat(),\n",
    "            'model_name': package_info['model_name'],\n",
    "            'model_path': package_info['model_path'],\n",
    "            'test_results': {\n",
    "                'all_passed': test_results['all_passed'],\n",
    "                'passed_count': test_results['passed_count'],\n",
    "                'total_count': test_results['total_count']\n",
    "            },\n",
    "            'performance_metrics': {\n",
    "                'test_r2': metrics['test_r2'],\n",
    "                'test_rmse': metrics['test_rmse'],\n",
    "                'test_mae': metrics['test_mae']\n",
    "            },\n",
    "            'status': 'ready_for_production'\n",
    "        }\n",
    "        \n",
    "        manifest_path = os.path.join(self.model_dir, f\"{package_info['model_name']}_manifest.json\")\n",
    "        with open(manifest_path, 'w') as f:\n",
    "            json.dump(manifest, f, indent=2)\n",
    "        \n",
    "        return manifest\n",
    "    \n",
    "    def execute(self, model, test_results, metrics, data_hash):\n",
    "        \"\"\"Executar pipeline de deployment\"\"\"\n",
    "        print(\"\\n=== EXECUTANDO DEPLOYMENT PIPELINE ===\")\n",
    "        \n",
    "        # Validar\n",
    "        if not self.validate_for_deployment(test_results):\n",
    "            print(\"\\n❌ Deployment abortado: modelo reprovou em testes\")\n",
    "            return None\n",
    "        \n",
    "        # Empacotar\n",
    "        metadata = {\n",
    "            'data_hash': data_hash,\n",
    "            'metrics': {k: float(v) for k, v in metrics.items()},\n",
    "            'creation_date': datetime.now().isoformat()\n",
    "        }\n",
    "        package_info = self.package_model(model, metadata)\n",
    "        \n",
    "        # Criar manifesto\n",
    "        manifest = self.create_deployment_manifest(package_info, test_results, metrics)\n",
    "        \n",
    "        print(\"[4/4] Deployment concluído!\")\n",
    "        print(f\"\\n✅ Modelo pronto para produção: {package_info['model_name']}\\n\")\n",
    "        \n",
    "        return {\n",
    "            'package_info': package_info,\n",
    "            'manifest': manifest\n",
    "        }\n",
    "\n",
    "# Executar deployment\n",
    "deployment_pipeline = DeploymentPipeline()\n",
    "deployment_result = deployment_pipeline.execute(\n",
    "    training_result['pipeline'],\n",
    "    test_results,\n",
    "    training_result['metrics'],\n",
    "    data['data_hash']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pipeline Completo Orquestrado\n",
    "\n",
    "### Tarefa 5: Integre todos os pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPipelineOrchestrator:\n",
    "    \"\"\"Orquestrador de pipeline completo de ML\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.results = {}\n",
    "    \n",
    "    def execute(self):\n",
    "        \"\"\"Executar pipeline completo\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"    INICIANDO PIPELINE COMPLETO DE ML\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            # 1. Data Pipeline\n",
    "            data_pipeline = DataPipeline(\n",
    "                test_size=self.config['test_size'],\n",
    "                random_state=self.config['random_state']\n",
    "            )\n",
    "            data = data_pipeline.execute()\n",
    "            self.results['data'] = data\n",
    "            \n",
    "            # 2. Training Pipeline\n",
    "            training_pipeline = TrainingPipeline(\n",
    "                experiment_name=self.config['experiment_name']\n",
    "            )\n",
    "            training_result = training_pipeline.execute(\n",
    "                data['X_train'], data['X_test'],\n",
    "                data['y_train'], data['y_test'],\n",
    "                self.config['model_params'],\n",
    "                data['data_hash']\n",
    "            )\n",
    "            self.results['training'] = training_result\n",
    "            \n",
    "            # 3. Test Pipeline\n",
    "            test_pipeline = ModelTestPipeline()\n",
    "            test_results = test_pipeline.execute(\n",
    "                training_result['pipeline'],\n",
    "                training_result['metrics'],\n",
    "                data['X_test'],\n",
    "                data['y_test']\n",
    "            )\n",
    "            self.results['tests'] = test_results\n",
    "            \n",
    "            # 4. Deployment Pipeline\n",
    "            deployment_pipeline = DeploymentPipeline(\n",
    "                model_dir=self.config['model_dir']\n",
    "            )\n",
    "            deployment_result = deployment_pipeline.execute(\n",
    "                training_result['pipeline'],\n",
    "                test_results,\n",
    "                training_result['metrics'],\n",
    "                data['data_hash']\n",
    "            )\n",
    "            self.results['deployment'] = deployment_result\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"    PIPELINE COMPLETO EXECUTADO COM SUCESSO\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            return self.results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ Erro no pipeline: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Configuração do pipeline\n",
    "config = {\n",
    "    'experiment_name': 'automated_cicd_pipeline',\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'model_params': {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'random_state': 42\n",
    "    },\n",
    "    'model_dir': '/tmp/deployed_models'\n",
    "}\n",
    "\n",
    "# Executar pipeline orquestrado\n",
    "orchestrator = MLPipelineOrchestrator(config)\n",
    "results = orchestrator.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Simulação de CI/CD com GitHub Actions\n",
    "\n",
    "### Tarefa 6: Crie arquivo de configuração CI/CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar exemplo de GitHub Actions workflow\n",
    "github_actions_workflow = \"\"\"\n",
    "name: ML Pipeline CI/CD\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "  schedule:\n",
    "    - cron: '0 0 * * 0'  # Semanal\n",
    "\n",
    "jobs:\n",
    "  train-and-deploy:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    \n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v2\n",
    "      with:\n",
    "        python-version: '3.8'\n",
    "    \n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        pip install -r requirements.txt\n",
    "    \n",
    "    - name: Run data pipeline\n",
    "      run: |\n",
    "        python scripts/data_pipeline.py\n",
    "    \n",
    "    - name: Train model\n",
    "      run: |\n",
    "        python scripts/train_model.py\n",
    "    \n",
    "    - name: Run tests\n",
    "      run: |\n",
    "        python -m pytest tests/\n",
    "    \n",
    "    - name: Deploy model\n",
    "      if: success()\n",
    "      run: |\n",
    "        python scripts/deploy_model.py\n",
    "      env:\n",
    "        MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}\n",
    "\"\"\"\n",
    "\n",
    "# Salvar workflow\n",
    "workflow_dir = \"/tmp/github_workflows\"\n",
    "os.makedirs(workflow_dir, exist_ok=True)\n",
    "with open(f\"{workflow_dir}/ml_pipeline.yml\", 'w') as f:\n",
    "    f.write(github_actions_workflow)\n",
    "\n",
    "print(\"GitHub Actions workflow criado:\")\n",
    "print(github_actions_workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Relatório de Execução\n",
    "\n",
    "### Tarefa 7: Gere relatório completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pipeline_report(results):\n",
    "    \"\"\"Gerar relatório completo do pipeline\"\"\"\n",
    "    \n",
    "    report = f\"\"\"\n",
    "╔══════════════════════════════════════════════════════════╗\n",
    "║           RELATÓRIO DE EXECUÇÃO DO PIPELINE             ║\n",
    "╚══════════════════════════════════════════════════════════╝\n",
    "\n",
    "Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "1. DATA PIPELINE\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Data Hash:       {results['data']['data_hash']}\n",
    "Train Samples:   {len(results['data']['X_train'])}\n",
    "Test Samples:    {len(results['data']['X_test'])}\n",
    "Features:        {results['data']['X_train'].shape[1]}\n",
    "\n",
    "Validações:\n",
    "\"\"\"\n",
    "    \n",
    "    for validation, passed in results['data']['validations'].items():\n",
    "        status = \"✅\" if passed else \"❌\"\n",
    "        report += f\"  {status} {validation}\\n\"\n",
    "    \n",
    "    metrics = results['training']['metrics']\n",
    "    report += f\"\"\"\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "2. TRAINING PIPELINE\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Run ID:          {results['training']['run_id']}\n",
    "\n",
    "Métricas:\n",
    "  Train RMSE:    {metrics['train_rmse']:.2f}\n",
    "  Test RMSE:     {metrics['test_rmse']:.2f}\n",
    "  Train R²:      {metrics['train_r2']:.4f}\n",
    "  Test R²:       {metrics['test_r2']:.4f}\n",
    "  CV RMSE:       {metrics['cv_rmse_mean']:.2f} ± {metrics['cv_rmse_std']:.2f}\n",
    "\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "3. TEST PIPELINE\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Testes Executados: {results['tests']['total_count']}\n",
    "Testes Aprovados:  {results['tests']['passed_count']}\n",
    "Status:            {'✅ APROVADO' if results['tests']['all_passed'] else '❌ REPROVADO'}\n",
    "\"\"\"\n",
    "    \n",
    "    if results['deployment']:\n",
    "        report += f\"\"\"\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "4. DEPLOYMENT PIPELINE\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Status:          ✅ DEPLOYED\n",
    "Model Name:      {results['deployment']['package_info']['model_name']}\n",
    "Model Path:      {results['deployment']['package_info']['model_path']}\n",
    "Deployment Time: {results['deployment']['manifest']['deployment_timestamp']}\n",
    "\"\"\"\n",
    "    else:\n",
    "        report += \"\"\"\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "4. DEPLOYMENT PIPELINE\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Status:          ❌ NOT DEPLOYED (testes falharam)\n",
    "\"\"\"\n",
    "    \n",
    "    report += \"\"\"\n",
    "╔══════════════════════════════════════════════════════════╗\n",
    "║                   FIM DO RELATÓRIO                       ║\n",
    "╚══════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Gerar relatório\n",
    "report = generate_pipeline_report(results)\n",
    "print(report)\n",
    "\n",
    "# Salvar relatório\n",
    "with open('/tmp/pipeline_report.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercícios Adicionais\n",
    "\n",
    "### Desafios para Praticar:\n",
    "\n",
    "1. **DVC Integration**: Use DVC para versionamento de dados:\n",
    "   ```bash\n",
    "   dvc init\n",
    "   dvc add data/\n",
    "   git add data.dvc .gitignore\n",
    "   ```\n",
    "\n",
    "2. **Pre-commit Hooks**: Configure hooks para validar código antes de commit\n",
    "3. **Docker**: Containerize todo o pipeline\n",
    "4. **Kubernetes**: Deploy o pipeline em cluster K8s\n",
    "5. **Airflow**: Orquestre o pipeline com Apache Airflow\n",
    "6. **Model Versioning**: Implemente versionamento semântico para modelos\n",
    "7. **Rollback Automático**: Implemente rollback se novo modelo falhar\n",
    "\n",
    "### Estrutura de Projeto Recomendada:\n",
    "\n",
    "```\n",
    "ml-project/\n",
    "├── .github/\n",
    "│   └── workflows/\n",
    "│       └── ml_pipeline.yml\n",
    "├── data/\n",
    "│   ├── raw/\n",
    "│   └── processed/\n",
    "├── models/\n",
    "├── notebooks/\n",
    "├── scripts/\n",
    "│   ├── data_pipeline.py\n",
    "│   ├── train_model.py\n",
    "│   ├── test_model.py\n",
    "│   └── deploy_model.py\n",
    "├── tests/\n",
    "│   ├── test_data.py\n",
    "│   ├── test_model.py\n",
    "│   └── test_api.py\n",
    "├── requirements.txt\n",
    "├── setup.py\n",
    "└── README.md\n",
    "```\n",
    "\n",
    "### Questões para Reflexão:\n",
    "\n",
    "1. Como garantir reprodutibilidade em pipelines de ML?\n",
    "2. Quais testes são essenciais antes de deployment?\n",
    "3. Como lidar com pipelines de longa duração?\n",
    "4. Quando usar orquestração vs. simples scripts?\n",
    "5. Como monitorar pipelines em produção?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
